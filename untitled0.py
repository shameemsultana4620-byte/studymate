# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IU9B-ed6W-fRECqpcv6-0-6ZQPYj6MZQ
"""

!pip install torch transformers accelerate langchain faiss-cpu PyPDF2

# Colab / Python notebook code

# Step 1: Install dependencies
!pip install torch transformers accelerate langchain faiss-cpu PyPDF2

# Step 2: Imports
import os
from google.colab import files
from PyPDF2 import PdfReader
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA

# Step 3: Load the Granite 3.2 2B Instruct model + tokenizer
model_name = "ibm-granite/granite-3.2-2b-instruct"
device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
)
model.eval()

# (Optional) If you want to enable “thinking” mode / reasoning mode, check model / tokenizer config
# The model’s docs suggest using a “thinking=True” argument to tokenizer.apply_chat_template. :contentReference[oaicite:0]{index=0}

# Step 4: Function to extract text from uploaded PDFs
def load_pdf_to_text(file_path: str) -> str:
    reader = PdfReader(file_path)
    pages = []
    for page in reader.pages:
        pages.append(page.extract_text())
    return "\n".join(pages)

# Step 5: Upload PDFs
uploaded = files.upload()
texts = []
for fname in uploaded:
    text = load_pdf_to_text(fname)
    texts.append(text)

# Step 6: Create Document objects and split into chunks
docs = [Document(page_content=t) for t in texts]
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = splitter.split_documents(docs)

# Step 7: Use embeddings + FAISS vector store
# Use a HuggingFace embeddings model; Granite may have embedding models but we can pick a generic one
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
hf_embed = HuggingFaceEmbeddings(model_name=embedding_model)
vector_store = FAISS.from_documents(split_docs, hf_embed)

# Step 8: Build a QA chain (retrieval + LLM)
retriever = vector_store.as_retriever(search_kwargs={"k": 4})

def ask_question(question: str) -> str:
    # Retrieve relevant chunks
    docs = retriever.get_relevant_documents(question)
    # Build a prompt combining context + question
    context = "\n\n".join([doc.page_content for doc in docs])
    prompt = f"""You are an AI academic assistant. Use the following context excerpts from PDFs to answer the question. If unsure, say “I’m sorry, I don’t know.”

Context:
{context}

Question: {question}

Answer (concise, with references):"""

    # Tokenize + generate
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    output_ids = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.0,
    )
    answer = tokenizer.decode(output_ids[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    return answer

# Step 9: Interactive loop
print("Ask questions about the uploaded PDFs (type 'exit' to quit).")
while True:
    q = input("You: ")
    if q.lower().strip() == "exit":
        break
    resp = ask_question(q)
    print("StudyMate:", resp)